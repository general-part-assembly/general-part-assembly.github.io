<!DOCTYPE HTML>
<html>
	<head>
		<title>General Part Assembly Transformer</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=1000">
		<link rel="stylesheet" href="assets/css/main.css" />

		<!-- Global site tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=G-VWXW8803RR"></script>
		<script>
			window.dataLayer = window.dataLayer || [];
			function gtag(){dataLayer.push(arguments);}
			gtag('js', new Date());

			gtag('config', 'G-VWXW8803RR');
		</script>

		<meta property="og:url"           content="todo:url" />
		<meta property="og:type"          content="website" />
		<meta property="og:title"         content="General Part Assembly Planning" />
		<meta property="og:description"   content="todo:abstract" />
		<script type="text/javascript" id="MathJax-script" async
			src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"> </script>
	
	</head>
	<body id="top">
		<!-- Main -->
			<div id="main" style="padding-bottom:1em; padding-top: 5em; width: 60em; max-width: 70em; margin-left: auto; margin-right: auto;">
					<section id="four">

						<h1 style="text-align: center; margin-bottom: 0;"><font color="4e79a7">General Part Assembly Planning</font> </h1>
						<!-- <h2 style="text-align: center; white-space: nowrap;">subtitle</h2> -->
						Most successes in autonomous robotic assembly have been restricted to single target or category. We propose to investigate general part assembly, the ability to assemble novel target shape with unseen part shapes.
						To tackle the planning of general part assembly, we present General Part Assembly Transformer (GPAT), a transformer-based model architecture that predicts part poses by inferring how each part corresponds to the target shape.
						Our experiments on both 3D CAD models and real-world scans demonstrate GPAT's generalization abilities to novel and diverse target and part shapes.
						
						<div style="text-align: center; margin-top: 1em">
							<img src="figs/teaser.gif" style="width: 70%;" alt="teaser">
						</div>

						<!-- <div style="display: flex; justify-content: space-between;">
							<img src="figs/table_animate.gif" style="width: 50%;" alt="teaser_table">
							<img src="figs/teaser.gif" style="width: 50%;" alt="teaser">
						</div>
						 -->
						<p>
						</p>
						
<!-- 						
						<hr/ style="margin-top: 1em">
				        <h3>Paper & Code</h3> -->

						<hr/ style="margin-top: 1em">
				        <h3>General Part Assembly Transformer (GPAT)</h3>
						We tackle the problem of general part assembly by target segmentation followed with pose estimation. To predict accurate segmentation of the target, a key challenge is to deal with the ambiguities in the target shape due to geometrically equivalent parts (e.g., legs of a table). To infer accurate segmentations, we propose General Part Assembly Transformer (GPAT), a transformer-based model architecture, that processes input shapes in a fine-to-coarse manner, thereby ensuring consistent segmentation results. 
						<div style="text-align: center; margin-top: 1em">
						<img src="figs/method.pdf" style="max-width: 100%;" alt="General Part Assembly Transformer (GPAT)">
						</div>

						<hr/ style="margin-top: 1em">
						<h3>Results</h3>
						For each test sample, our model takes as inputs a target shape and variable set of part shapes. The target shape may be given at either a caonical pose or a random pose, and part shapes may be exact or non-exact match. Chairs, lamps, and faucets are seen during the training, while tables and displays are unseen categories. The first row of each category displays targets in black, and the second row shows the model predictions.

						
						<div style="text-align: center; margin-top: 1em">
						<img src="figs/sup.pdf" style="width: 100%" alt="Results">
						</div>
						
						<hr/ style="margin-top: 1em">
						<h3>Results with Real-world Scans as Targets</h3>
						Each row shows predictions given 3 sets of part shapes and the same real-world scan as the target shape.
						<div style="text-align: center; margin-top: 1em">
						<img src="figs/realworld_all.pdf" style="width: 70%" alt="Results with Real-world Scans">
						</div>


						<!-- <hr/ style="margin-top: 1em">
						<h3>Multi-scale Attention Layer</h3>
						TF denotes an alternative segmentation-based model that uses the vanilla transformer, which fails to produce consistent segmentations for targets with geometrically equivalent parts. With the multi-scale attention layer, GPAT fully leverages the spatial structure of the target point clouds to produce consistent segmentations and accurate assemblies.
						<div style="text-align: center; margin-top: 1em">
						<img src="figs/seg.pdf" style="width: 70%" alt="Results with Real-world Scans">
						</div> -->

						<!-- <hr/ style="margin-top: 1em">
						<h3>The Effect of Optimization</h3>
						<div style="text-align: center;">
						<img src="figs/opt.pdf" style="width: 100%" alt="The Effect of Optimization">
						</div> -->

						<hr/ style="margin-top: 1em">
						<h3>Application to Part Discovery</h3>
						GPAT is directly applicable to the task of part discovery, \ie, predict a part segmentation given a target, if we do not provide input parts. 
						We show some qualitative results in the figure below to test GPAT's part discovery abilities.
						Given non-exact parts, PAT predicts accurate segmentations as usual. If we input identical blocks, which specifies the number of parts but provides little information about the part shapes, then GPAT predicts reasonable segmentations with the specified number of segments. 
						Finally, we omit the input parts, and GPAT successfully discovers parts in the target shape.
						<div style="text-align: center; margin-top: 1em">
						<img src="figs/feature.pdf" style="width: 70%" alt="Application to Part Discovery">
						</div>

						<hr/ style="margin-top: 1em">
						<h3>Sensitivity to Scale</h3>
						Part assembly often involves parts that have the same geometry but different scales, so it is necessary for a model to discriminate parts of different scales to create correct assemblies.  As an qualitative illustration in the figure below, we adjust the scale of the parts that have same geometry (the legs of chair/table), and the model correctly associates parts of different scales to the target to build the desired assemblies.
						<div style="text-align: center; margin-top: 1em">
						<img src="figs/scale.pdf" style="width: 70%" alt="Application to Part Discovery">
						</div>

						<hr/ style="margin-top: 1em">
						<h3>Failure Modes</h3>
						GPAT is not without limitations, and the figure below shows some typical failure cases. First, GPAT tends to give incorrect segmentation predictions if some parts are hidden inside a larger part (e.g., the light bulbs in a lamp) or the parts are less separable (e.g., overlapping parts of a microwave). To solve with these issues, it is possible to introduce additional information like colors and normals of the point clouds as inputs. 
						Additionally, oriented bounding box can be insufficient as a pose estimation method for some parts. To tackle this problem, a learning-based pose estimation module can potentially replace the bounding box procedure.
						<div style="text-align: center; margin-top: 1em">
						<img src="figs/failure.pdf" style="width: 70%" alt="Failure Modes">
						</div>

						<!-- <hr/ style="margin-top: 1em">
				        <h3>Algorithm Comparisons</h3>
						<div style="text-align: center;">
						<img src="figs/main.pdf" style="width: 100%" alt="Algorithm Comparisons">
						</div> -->
						
						<!-- <hr/ style="margin-top: 1em">
				        <h3>Team</h3> -->
						
						<!-- <hr/ style="margin-top: 1em">
				        <h3>Acknowledgements</h3>
				        <p>
						 The authors would like to thank Zhenjia Xu, Samir Gadre, Huy Ha, Chi Cheng, Xiaolong Li, Yifan You, and Boyuan Chen for their help throughout the project. We would also thank Google for their donation of UR5 robots. This work was supported in part by the National Science Foundation under CMMI-2037101 and Amazon Research Award. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.
						</p> -->

						<!-- <hr/ style="margin-top: 1em">
                        <h3>Citation</h3>
                        To cite this work, please use the following BibTex entry,
                        <pre>
                        <code>@inproceedings{yulong2022scene,
  title={Scene Editing as Teleoperation: A Case Study in 6DoF Kit Assembly},
  author={Li, Yulong, and Agrawal, Shubham, and Liu, Jen-Shuo, and Feiner, Steven, and Song, Shuran},
  booktitle={2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year={2022},
  organization={IEEE}
}
</code>
</pre> -->

				        <!-- <p>If you have any questions, please feel free to contact 
							<a href="http://www.columbia.edu/~yl4095/">Yulong Li</a>.</p> -->
						<hr/>
			</div>

		<!-- Footer -->
			<!-- <footer id="footer">
				<div class="inner">
					<ul class="copyright">
						<li>Meet <a href="https://en.wikipedia.org/wiki/Danbo_(character)">Danbo</a> the cardboard robot.</li>
					</ul>
				</div>
			</footer> -->

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.poptrox.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>
	</body>
</html>
